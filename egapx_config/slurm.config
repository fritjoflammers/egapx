// Config for typical SLURM cluster
// Adjust the parameters for your cluster - queue name, temp dir, etc.
// Use your temp space instead of /data/$USER in the following config

singularity {
    enabled = true
    autoMounts = true
    cacheDir = "/cluster/scratch/flamme/singularity/cache"
    runOptions = "--bind /cluster/scratch/flamme:/cluster/scratch/flamme --bind /cluster/scratch/flamme:/scratch"
    envWhitelist='https_proxy,http_proxy,ftp_proxy,DISPLAY,SLURM_JOB_ID,SINGULARITY_BINDPATH,SCRATCH'
}

env {
    SINGULARITY_CACHEDIR="/cluster/scratch/flamme/singularity/cache"
    SINGULARITY_TMPDIR="/cluster/scratch/flamme/singularity/tmp"

    DEBUG_STACK_TRACE_LEVEL="Warning"
    EXCEPTION_STACK_TRACE_LEVEL="Warning"
    DIAG_POST_LEVEL="Trace"
    DEBUG_CATCH_UNHANDLED_EXCEPTIONS="0"

// these turn off connecting to local NCBI services, it can simulate running without certain network access
// CONN_DISPD_DISABLE="1"
// CONN_LBSMD_DISABLE="1"

  // override node scratch that is inaccesible for singularity
  TMPDIR = '/cluster/scratch/flamme'

}

executor {
   perCpuMemAllocation = true
}
process {
    executor = 'slurm'
    queueSize = 20
    retry.maxAttempt = 1


    // Set other parameters specific to your cluser here
    maxRetries = 1
    // queue = 'norm'
    // queueSize = 200
    // pollInterval = '2 min'
    // queueStatInterval = '5 min'
    submitRateLimit = '6/1min'
    // retry.maxAttempts = 1

    // clusterOptions = ' --gres=lscratch:200 '

    
    // override node scratch that is inaccesible for singularity
    env.TMPDIR = '/cluster/scratch/flamme'

    // scratch = '/lscratch/$SLURM_JOB_ID'
    // with the default stageIn and stageOut settings using scratch can
    // result in humungous work folders
    // see https://github.com/nextflow-io/nextflow/issues/961 and
    //     https://www.nextflow.io/docs/latest/process.html?highlight=stageinmode
    stageInMode = 'symlink'
    stageOutMode = 'copy'
}

